[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Experiments in Data Science, AI, and programming."
  },
  {
    "objectID": "posts/1_keyboard_mouse/index.html",
    "href": "posts/1_keyboard_mouse/index.html",
    "title": "Your Keyboard As A Mouse",
    "section": "",
    "text": "Setting up a Raspberry Pi can be frustrating if you’re not prepared. On the manual route of flashing an SD card and navigating the alpha soup of cable connectors (USB Mini, USB C, HDMI to Mini HDMI), the other potential sticking points are the input and output devices. So it was that one day I dodged all the potential bullets before being painfully shot with a missing mouse. “If only I could use a keyboard as a mouse,” put me down the road that led to this article.\nFirst I tried synthesizing this keyboard enhancement drug in Python with pyinput and pyautogui. That worked in testing but I couldn’t get it to work with systemd as a permanent application. Someone helpfully pointed out that there were some lower level shenanigans going on, but I wasn’t sure how to proceed so I abandoned this work for a while.\nI forget the catalyst for its revival, but somehow I came across the exact architecture I needed to use: 1 2. xbindkeys and xte became the keys to my success.\nsudo apt update\nsudo apt-get install xbindkeys -y\nsudo apt-get install xautomation -y\ntouch ~/.xbindkeysrc\nxbindkeys --key # find name of key\nkillall -s1 xbindkeys # make xbindkeys reload config\nxbindkeys # start xbindkeys daemon\nxbindkeys -f ~/.xbindkeysrc\nAnd inside the .xbindkeysrc config file:\n# Keyboard as Mouse\n\n# move mouse up (with alt+w)\n\"xte 'mousermove 0 -10'\"\n   alt+w\n\n# move mouse down\n\"xte 'mousermove 0 10'\"\n   alt+s\n\n# move mouse left\n\"xte 'mousermove -10 0'\"\n   alt+a\n\n# move mouse right\n\"xte 'mousermove 10 0'\"\n   alt+d\n\n# left click\n\"sleep 1 && xte 'mouseclick 1'\"\n   alt+q\n\n# right click\n\"sleep 1 && xte 'mouseclick 3'\"\n   alt+e"
  },
  {
    "objectID": "posts/7_subttitles_on_the_go/index.html",
    "href": "posts/7_subttitles_on_the_go/index.html",
    "title": "Subtitles On The Go",
    "section": "",
    "text": "Download the video from Youtube:\nyt-dlp \"https://www.youtube.com/watch?v=q4tpg6mkBTs\"\nExtract the audio (I changed the name of the file to nims.webm):\nffmpeg -i nims.webm -vn -ab 128k -ar 44100 -y nims.mp3\nSetup up OpenAI Whisper…OK, so this is non-trivial. For my workflow I’m renting an A100-80G Linux Desktop from Paperspace. I log into the machine and ran sudo apt update so I can install pip and git. Then I installed Whisper (https://github.com/openai/whisper):\npip install git+https://github.com/openai/whisper.git \nThere are numerous ways to get the video on the other machine. I chose to send it over with magic wormhole.\nI ran the default script provided in the repo:\nimport whisper\n\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])\nGot a transcript in Japanese and posted it into DeepL for a translation. It was decent but with obvious mistakes even for someone who doesn’t speak Japanese. I know Whisper should be able to do translation so I just had to figure it out.\nTurns out the cli has a lot of params one of which is translation. Bad news was my cli installation wasn’t working. I had to figure out how do to it in Python. Or copy-paste which is what I did after searching the Whisper repo’s Discussion page. With that I cut DeepL out of the pipeline, what’s more the translation was even better.\nNext I needed actual subtitles - another feature tied to the cli installation. I ended up copying and modifying two utility scripts from the Whisper AI repo into my own to produce an STR (subtitle) file. This is my final script:\nimport whisper\n\ndef format_timestamp(seconds: float, always_include_hours: bool = False, decimal_marker: str = '.'):\n    assert seconds &gt;= 0, \"non-negative timestamp expected\"\n    milliseconds = round(seconds * 1000.0)\n\n    hours = milliseconds // 3_600_000\n    milliseconds -= hours * 3_600_000\n\n    minutes = milliseconds // 60_000\n    milliseconds -= minutes * 60_000\n\n    seconds = milliseconds // 1_000\n    milliseconds -= seconds * 1_000\n\n    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours &gt; 0 else \"\"\n    return f\"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n\nmodel = whisper.load_model(\"large\")\nresult = model.transcribe(\"nims.mp3\", task=\"translate\")\n#print(result[\"text\"])\n\nwith open(\"script.srt\", \"w+\") as f:\n\n  for i, segment in enumerate(result['segments'], start=1):\n    print(\n            f\"{i}\\n\"\n            f\"{format_timestamp(segment['start'], always_include_hours=True, decimal_marker=',')} --&gt; \"\n            f\"{format_timestamp(segment['end'], always_include_hours=True, decimal_marker=',')}\\n\"\n            f\"{segment['text'].strip().replace('--&gt;', '-&gt;')}\\n\",\n            file = f,\n            flush = True,\n        )\nI sent the script back to my computer and then called up ffmpeg to do the honors of generating the video with subtitles:\nffmpeg -i nims.webn -vf subtitles=nims_script.srt output.mp4\nThere’s a popular AI YouTube channel, Two Minutes Papers, where the host has his own catchphrase, “What a time to be alive!” That’s what I said after being able to watch and understand what was going in the video. (For QA it helped that the subject and topic was obvious.) It took me a while to get the initial pipeline done, but now that its set, I can probably go from video to video with English subtitles in less than 5 minutes - controlling for the size of the video.\nWhat a time to be alive."
  },
  {
    "objectID": "posts/10_text_an_ai/index.html",
    "href": "posts/10_text_an_ai/index.html",
    "title": "Text An AI",
    "section": "",
    "text": "My last AI post was back on October 14. It seems like a lifetime ago. Back then I was dipping my toe into the vast ocean created by the release of ChatGPT. At one point I had even canceled my subscription due to lack of use. Now I’m scuba diving daily, and this is one gem I found on the sea floor.\nSorry, I’m exaggerating. I only use ChatGPT or OpenAI on the desktop since apps aren’t really my thing. Recently while thinking of multi-channel engagement of AI powered chat bots, I wondered if I could text an LLM, and have it answer back. Easier said then done. The new texting law to cut down on spam - 10DLC or something - killed hobby use of SMS. I got booted off Openphone despite only using it for receiving SMS codes, and an attempt to create a new Twilio received a hard no. Increasingly the world of SMS is just for businesses.\nThere is a glimmer of hope, though, in the least unexpected of places. Here comes email to the rescue.\nYou can text people through email with their phone number and carrier. For example, 000-000-0000@txt.att.net for people using AT&T. The reverse is also true. You can send a text to an email address where it will appear as an email.\nMy first step was to figure out how to send and parse received email programmatically. (As a quick note you need to create an app password to do both with code.) Years ago I used an library called etext for going from email to text, but it doesn’t handle the reverse. I briefly thought about using the Gmail API, before remembering how much I loathe interacting with most of Google’s APIs.\nA lot of googling reminded me that really only programmatic outbound mail seems to matter from a business perspective. Finally, consulting the classic book “Automate The Boring Things With Python”finally led me in the right direction, and an actual cursory understanding of how email works.\nLong story short, the email protocol has two components: smtplib for sending, and imaplib for receiving. Both of which are standard Python libraries. Writing code for sending was a breeze. Doing the same for parsing received email was not. Eventually I got here, and text to my email:\n“!oai what is the capital of france?”\n“!oai” is the trigger phrase that tells the script to send the content to the OpenAI API instead of ignoring it. Then I ran the script and received the reply:\n“The capital of France is Paris”\nThat was enough to satisfy my curiosity. For a full implementation I would deploy this on Modal polling every minute:\nhttps://modal.com/docs/guide/cron#basic-scheduling\nBut as I said, I really only use AI on my computer. Here is the full code:\nhttps://github.com/gfleetwood/automation/blob/root/python/email_sms_ai.py"
  },
  {
    "objectID": "posts/5_colored_equations/index.html",
    "href": "posts/5_colored_equations/index.html",
    "title": "Colored Equations",
    "section": "",
    "text": "I had this laying around as a gist and decided to include it here. See the source here:\nhttps://github.com/gfleetwood/gfleetwood.github.io/blob/master/index.Rmd\nIn particular pay attention to the LaTeX packages in the yaml header (header-includes:).\n\\(\\color{blue}{y} = \\color{green}{m}\\color{red}{x} + \\color{orange}{b}\\)\nFor every one unit change in the independent variable there is a change in the dependent variable plus some offset/bias to represent the value of the dependent variable when the independent variable is zero.\nFor example, consider a mock linear relationship between a person’s weight in pounds and their height in inches.\n\\(\\color{blue}{Weight} = \\color{green}{2}*\\color{red}{Height} + \\color{orange}{12}\\)\nThis says that it is expected that a person’s weight will increase by 2 pounds for every additional inch in their height. If a person’s height were 0 inches, then their weight would be 12 pounds.\nFrom DataCamp’s Bayesian course\n\\(P(\\color{orange}{\\theta}|\\color{purple}{D}) = \\frac{\\color{green}{P(D|\\theta)}\\times \\color{blue}{P(\\theta})}{\\color{red}{\\Sigma{P(D|\\theta)} \\times P(\\theta)}}\\)\n\nThe probability of different parameter values given some data is the likelihood (The relative probability of the data given different parameter values) multiplied by the prior (The probability of different parameters before seeing the data) divided by the total sum of the likelihood weighted by the prior."
  },
  {
    "objectID": "posts/6_truth_tables/index.html",
    "href": "posts/6_truth_tables/index.html",
    "title": "Propositional Logic In Python",
    "section": "",
    "text": "Recently I wanted to compare two truth tables. After some frustrating googling for a simple site to use, I decided to try Python. It was easy:\nfrom itertools import product\nfrom pandas import DataFrame\n \nexp = lambda a, b, c, d, e: a and b or c and d and e\nexp2 = lambda a, b, c, d, e: a and (b or c) and d and e\n \ndef exp3(a, b, c, d, e): \n \n  result = a and b or c and d and e\n  truth_table = {\"a\": a, \"b\": b, \"c\": c, \"d\": d, \"e\": e, \"a & b | c & d & e\": result}\n \n  return(truth_table)\n \narr = [False, True]\ncartesian_product = list(product(arr, arr, arr, arr, arr))\n \n#for a,b in zip(list(map(lambda x: exp(*x), cartesian_product)), list(map(lambda x: exp2(*x), cartesian_product))):\n#  print(a == b)\n \ntruth_table = list(map(lambda x: exp3(*x), cartesian_product))\ntruth_table_df = DataFrame(truth_table)\n \ntruth_table_df"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "“Rise Above Yourself And Grasp The World”",
    "section": "",
    "text": "Text An AI\n\n\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nxG Experiments\n\n\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Steps In ChatGPT Vision\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSubtitles On The Go\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPropositional Logic In Python\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nColored Equations\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Audio Books\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Local DNS Websites\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nYour Keyboard As A Mouse\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAutomating Wordle\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/8_chatgpt_vision_first_try/index.html",
    "href": "posts/8_chatgpt_vision_first_try/index.html",
    "title": "First Steps In ChatGPT Vision",
    "section": "",
    "text": "It’s been a while. Almost a year in fact. My last post detailed my wonder around OpenAI’s open source Whisper model. In the year since then the AI world has continued on its breakneck pace.\nRecently I got access to ChatGPT’s Vision module and its been interesting. Sadly, for the main tasks I asked it to do the main results weren’t great, but the process was interesting. Here are a couple.\n1. Wordle\nI asked ChatGPT to play Wordle. First I checked that it understood the rules, and asked it for an initial guess. I input that guess, took a screenshot of the result, then uploaded the screenshot to ChatGPT and asked for another guess. This process repeated for six guesses.\nChatGPT lost and lost badly. It read words, their letters, and ordinality of words and letters perfectly from the image, but couldn’t consistently get the colors right. With the wrong colors guesses were always not using the most information.\nFunnily enough the first time I tried I uploaded a nearly complete Wordle image and ChatGPT had the colors right.\n2. Manga Translation\nI found this page of a manga with the translation in the comments:\nhttps://www.reddit.com/r/translator/comments/bjl4xr/japanese_english_extra_page_from_a_manga_volume/\nChatGPT gave me this when I asked it to translate:\nThe image contains several panels from a manga, each with Japanese text. I'll translate the content for you:\n\nTop Left Character: Drinking tea without any dialogue\n\nTop Right Character: \"This tea! It's really delicious!\"\n\nMiddle Left Character: \"It's super hot!\"\n\nMiddle Right Character: \"It's so good!\"\n\nBottom Left Female Character: \"Delicious!!\"\n\nBottom Right Elderly Character: \"Please drink slowly and enjoy it.\"\n\nI've provided a general translation, and there might be nuances based on the context of the story, but this should give you a basic understanding of what's being said.\nWay off. It’s almost like it was guessing what the conversation should be. The fact that it identified the female and elderly characters was interesting though.\n3. Extracting Data From Charts\nA while back I did a deep dive into tools for getting data from images. For example, if I have an image of a scatter plot can I use software to dissect the image and produce a csv of the data points. I wasn’t too impressed when the tools I found.\nUsing ChatGPT didn’t change anything. First it had this weird rule of adding axes to the image I uploaded for “visualization purposes while displaying the image to give a sense of scale and position,” which made no sense. Then it would said it couldn’t detect the correct axes. I ended up telling it the range of the x and y axes and it still messed up.\nIt did do well with a pie chart that had a legend with the labels and percentages."
  },
  {
    "objectID": "posts/2_post-with-code/index.html",
    "href": "posts/2_post-with-code/index.html",
    "title": "Automatic Audio Books",
    "section": "",
    "text": "Ideas on my “Someday” list usually spend a couple of weeks before leaving forever or returning in disgrace (mine not its). “Can I automatically create an audiobook?” had a successful turn around of about three days.\nThe book I choose was Carroll Quigley’s “The Evolution Of Civilizations”. I’m at a point where I am militantly electronic with my reading, so the pdf link from Twitter was perfect. Skimming the book I got the sense that I’d prefer to listen rather than read it - a bifurcation that I recently adopted after ignoring audiobooks for years. No audiobook on Amazon’s left me only one option. (Well there was the possibility of using an end to end implementation like Speechify but where would be the fun in that?)\nThree days from idea to completion is the small, visible part of an enormous glacier. Beneath the surface lies hours of previous research into pdf extraction and text to speech (also speech to text) programs. The pipeline and its implementation almost immediately existed as one entity in my mind. Here was my process:\n\nUse pdf extraction to get the text\n\nlibrary(pdftools)\n\ntext_extracted &lt;- pdf_text(\"auto-audio-book/CarrollQuigley-TheEvolutionOfCivilizations-AnIntroductionToHistoricalAnalysis1979.pdf\")\n\ntext_concatenated &lt;- paste(text_extracted, collapse = \" \")\ncon &lt;- file(\"evol_of_civilizations.txt\")\nwriteLines(text_concatenated, con)\nclose(con)\n\nCheck.\n\nClean the text\n\nSo I lied about the automatically part. I still had to go into the document and delete footers, references, the table of contents, etc, and I left stuff in there because it was too tedious to go through a 400+ page book to get every single one. Much easier to ignore them while listening. Alternatively this could be done after the text to speech conversion using a service like descript.\n\nUse text to speech to create the audio\n\nThis turned out to be more of a hassle than I thought it would be. First I went through Google’s convoluted process to get the Text To Speech API working. Then my attempt at making a single mp3 was thwarted by the API’s limits. Chunking the file and adding in a delay (*What felt like two hours later*) left me with 180 separate mp3s. (In retrospect I should have used the multiprocess module to speed it up.)\nfrom google.cloud import texttospeech\nimport time\n\ndef tts_book(ind, text_chunk):\n    # Instantiates a client\n    client = texttospeech.TextToSpeechClient()\n\n    # Set the text input to be synthesized\n    synthesis_input = texttospeech.types.SynthesisInput(text = text_chunk)\n\n    # Build the voice request, select the language code (\"en-US\") and the ssml\n    # voice gender (\"neutral\")\n    voice = texttospeech.types.VoiceSelectionParams(\n        language_code = 'en-US-Wavenet-B',\n        ssml_gender = texttospeech.enums.SsmlVoiceGender.NEUTRAL\n        )\n\n    # Select the type of audio file you want returned\n    audio_config = texttospeech.types.AudioConfig(\n        audio_encoding = texttospeech.enums.AudioEncoding.MP3\n        )\n\n    # Perform the text-to-speech request on the text input with the selected\n    # voice parameters and audio file type\n    response = client.synthesize_speech(synthesis_input, voice, audio_config)\n\n    # The response's audio_content is binary.\n    with open('output_{}.mp3'.format(ind), 'wb') as out:\n        # Write the response to the output file.\n        out.write(response.audio_content)\n        print('Audio content written to file \"output_{}.mp3\"'.format(ind))\n\n    return(1)\n\nwith open(\"auto-audio-book/evol-of-civilizations-cleaned.txt\", \"r\") as f:\n    evol_civ = f.read().replace(\"\\\\n\", \" \")\n\nchunks = []\ntemp = evol_civ\n\nwhile len(temp) != 0:\n\n    chunks.append(temp[:4000])\n    temp = temp[4000:]\n\nfor ind, chunk in enumerate(chunks):\n    _ = tts_book(ind, chunk)\n    time.sleep(10)\n\nIt was a pretty small wrench though. I knew ffmpeg was the solution I needed. The tricky bit was in getting it to work. After some Googling I assembled the parts:\n# Get all the mp3 names and write them to a text file.\nls | grep \"output\" &gt; mp3-files.txt\n\n# Prepend the word \"file\" to each line of said text file.\nawk '{print \"file \" $0}' mp3-files.txt &gt; mp3-files.txt\n\n# Bring in the heavy guns to produce the combined file\nffmpeg -f concat -safe 0 -i mp3-files.txt -c copy output-final.mp3\n\nSuccess!"
  },
  {
    "objectID": "posts/3_local_dns_websites/index.html",
    "href": "posts/3_local_dns_websites/index.html",
    "title": "Building Local DNS Websites",
    "section": "",
    "text": "This is a reference blog on how to build a local DNS.\nI use Google Chrome as my main browser, but I’ve been thinking of switching (or at least trying out) others. This possibility is hampered by Chrome lock-in powered by extensions and bookmarks, with the latter being the main issue.\nThe initial step involved a browser agnostic bookmark manager. I settled on buku which is cli based with an additional buku server to allow administration through a web interface. The next step was to permanently run buku server on a local port. Easy enough with systemd. Instead of bookmarking or typing 127.0.0.8000 (for example), I wanted to be able to type buku.me instead. That turned out to be much harder.\nAll these instructions pertain to Ubuntu 20.04.\nStep 1:\nInstall buku And buku server: pip3 install buku[server].\nStep 2:\nPermanently run buku server with system d.\nAdd this script as an executable to your bin folder. (Mine is called buku_server.) You’ll have to find out where buku server is installed with which bukuserver.\n# I'm using port 5555 but choose whichever port you want\n/home/gordon/miniconda3/bin/bukuserver run --host 127.0.0.1 --port 5555\n\nNext add this service file, buku_server.service, to /etc/systemd/system.\n[Unit]\nDescription=Buku Server\n\n[Service]\nUser=gordon\nWorkingDirectory=/home/gordon/.local/mybin\nExecStart=/bin/bash /home/gordon/.local/mybin/buku_server\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n\nStart and enable the service.\nsystemctl start buku_server.service\nsystemctl enable buku_server.service\n\nGo to 127.0.0.1:5555 to check that buku server is running.\nStep 3:\nThis was the draw the rest of the owl part for me. I finally understood the process due to this blog:\nhttps://www.interserver.net/tips/kb/local-domain-names-ubuntu/\nFirst you need to add a line to your /etc/hosts file: 127.0.0.1 buku.me. Then add this to a configuration file in /etc/apache2/sites-available (mine is 000-default.conf):\n# ServerName, ProxyPass, and ProxyPasReverse are the entries to change\n\n&lt;VirtualHost *:80&gt;\n  ServerAdmin test@test.com\n  ServerName buku.me\n  ProxyPass / &lt;http://127.0.0.1:5555/&gt;\n  ProxyPassReverse / &lt;http://127.0.0.1:5555/&gt;\n&lt;/VirtualHost&gt;\n\nWith that I was able to go to buku.me in my browser to access the buku server.\nBonus:\nEven with this setup I still had to manually add urls to buku server. I wanted to have a shortcut, CTRL+B, that would automatically add a url. This solution actually adds whatever is on the clipboard to buku, so it’s on me to make sure it’s a url.\nIn Ubuntu I mapped this custom shortcut to CTRL+B: home/gordon/buku_add.py where buku_add.py is:\nimport pyperclip\nimport os\n\nos.system(\"buku -a {}\".format(pyperclip.paste()))\n\nbuku_add.py is a vanilla Python script instead of an executable because that didn’t work. I have no idea why."
  },
  {
    "objectID": "posts/9_xg/index.html",
    "href": "posts/9_xg/index.html",
    "title": "xG Experiments",
    "section": "",
    "text": "Expected Goals is the premier stat in the world of soccer analytics. After struggling to make the jump from research to the media, it’s now a staple statistics in every broadcast.\nBut the some of the early pushback still remains, and it’s often quoted as an afterthought and without much comprehension. I thought a good idea would be to incorporate it into highlights to reinforce what it means.\nThe table below is the first five rows xG data from Burnley x Man City in the 2023-24 season.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMinute\nPlayer…2\nSquad\nxG\nPSxG\nOutcome\nDistance\nBody Part\nNotes\nPlayer…10\nEvent…11\nPlayer…12\nEvent…13\n\n\n\n\n4\nErling Haaland\nManchester City\n0.17\n0.62\nGoal\n8\nLeft Foot\nVolley\nRodri\nPass (Live)\nKevin De Bruyne\nPass (Live)\n\n\n6\nKevin De Bruyne\nManchester City\n0.02\n0.00\nOff Target\n32\nRight Foot\nNA\nPhil Foden\nPass (Live)\nNathan Aké\nPass (Live)\n\n\n11\nErling Haaland\nManchester City\n0.21\n0.00\nOff Target\n6\nRight Foot\nVolley\nKevin De Bruyne\nPass (Live)\nRodri\nPass (Live)\n\n\n15\nLuca Koleosho\nBurnley\n0.08\n0.00\nOff Target\n15\nRight Foot\nNA\nVitinho\nPass (Live)\nLyle Foster\nPass (Live)\n\n\n18\nZeki Amdouni\nBurnley\n0.03\n0.02\nSaved\n15\nLeft Foot\nNA\nLuca Koleosho\nPass (Live)\nVitinho\nPass (Live)\n\n\n\n\n\nSee the still images resulting from my poor video editing.\n \n\nI also made a couple of plots of to explore how to represent xG data. This is from fake data I generated.\n\n\nFinally, I made a game! It’s just a demo. It asks the player to watch a goal and guess what the xG of the shot was. The player’s score is determined by the Brier score. Unfortunately, I can’t keep it deploy right now, but here’s the code if you want to deploy and play it:\nhttps://github.com/gfleetwood/the_xg_game"
  },
  {
    "objectID": "posts/4_automating_wordle/index.html",
    "href": "posts/4_automating_wordle/index.html",
    "title": "Automating Wordle",
    "section": "",
    "text": "Recently I stumbled upon the Wordle craze, and lost interest after trying to solve them manually for one or two days. Programmatic attempts held my attention for much longer as I stumbled upon new ways to implement solutions. The most mind-catching one was a method using Z3:\nhttps://typon.github.io/wordle.html\nMy armchair fascination with formal methods in computer science often crossed paths with SAT solvers, but I never really gave them more than a cursory look for the depth involved. This seemed like a good opportunity to dive a bit deeper.\nBut that scope only really involved reading the blog and maybe running some lines of code. I need a deeper hook. One eventually came to mind. All the programmatic solutions were only really partially automated. A user would run code, see the wordle feedback, and then make adjustments as needed. I wanted to eliminate the need for adjustments. Everything would be automated.\nHow to begin. Well, some light RPA was key. I’m not a fan since I think the field is a walking anti-pattern, but it does have its uses. The first snippet of code opened a link to a Wordle clone without the once a day playing limit (https://hellowordl.net/), and used Python’s pyautogui to enter letters and take a screenshot. Some fiddling around with cropping produced this:\nNot bad. Next was the process of detecting letter and colors. Each square is about 55 pixels length and width, so a simple double for loop sufficed. Some googling helped me write this function to get the dominant color in a square:\ndef get_dominant_color(letter_square):\n\n  img = Image.fromarray(letter_square)\n  dom_color = sorted(img.getcolors(2 ** 24), reverse = True)[0][1]\n\n  return(dom_color)\nWhich I saved as a variable:\nCOLORS = [\n(\"yellow\", (228, 218, 0)), \n(\"gray\", (162, 162, 162)), \n(\"green\", (63, 186, 119))\n]\nOf course trying to match colors exactly is just calling for frustration. I used a distance metric instead.\nNext I wanted to use OCR to identify the letters. This easily took up most of my time. Through trial and error I came up with a computer vision pipeline that cleaned up the image above so much that a blind man could see which letters the image contained. Unfortunately neither pytesseract nor the Google Cloud API could. Despair began to set in.\nI’m sure you’re now confused. “Why OCR? Aren’t the letters already known since you’re entering the words?” I eventually came to realization as well, and jettisoned (read saved for reference) all that computer vision code.\nI went some way towards writing my own non-Z3 solver before remember my initial goal. Instead I copied over the relevant Z3 code from https://typon.github.io/wordle.html and worked to integrate it into my framework.\nMy big addition was the logic of how and when to add constraints to the solver. There are seven, two of which are static:\n\nOnly 26 letters (0-25)\nOnly five letter words (sourced from Linux’s dictionary)\n\nThe other five are dynamic. In parentheses are the color/s they pertain to:\n\nWord contains letter (green, yellow)\nWrong position in word for letter (yellow)\nRight position in word for letter (green)\nWord doesn’t contain letter (gray)\nWord contains only one instance of the letter (green, yellow)\n\nThe last one was the stickler. When the code is being manually updated it’s easy to enough to wield, but in a fully automated setup it was just breaking. Consider a word like flood. If I guessed flodd the first d would be gray and the other green. With z3 this would adds constraints that d is both in and not in the word. You immediately see why this is a problem. I need to check if a letter that should be considered was already labeled as not being in the word and then remove that constraint.\nThis, apparently, is not trivial in z3. The closest method is solver.remove() which removes all constraints from the solver. Not ideal. Armed with my accumulated z3 resources I finally delved deep into its workings, and arrived at an answer. I added this snippet of code to the cases where the letter was green or yellow:\ncheck_if_grey_before = f\"letter_{str(letter_info[2])} != {letter_to_index_map[letter_info[0]]}\"\nconstraints = solver.assertions()\nnew_constraints = [\nconstraint \nfor constraint in constraints \nif constraint.__repr__() != check_if_grey_before\n]\n\nif len(constraints) != len(new_constraints): \n  solver.reset()\n  solver.add(new_constraints)\nIn a nutshell it checks if the current constraints say the letter wasn’t in the word. If yes, then it filters out that constraint, resets the solver, and then adds back the constraints. I also added logic to the gray condition to skip saying a letter wasn’t in the word if it was already added as being in it.\nMy solution worked well sometimes, but the solver still broke down. Obviously I attributed this to some wonky logic in updating the solver, so I added logging code to see what constraints are being used on each turn. Looking at the logic led me to the true culprit. Turns out my color detection was not as ironclad as I thought.\nHere’s the full code: gfleetwood/auto-wordle"
  }
]