---
title: "Blog"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    toc_collapsed: true
toc_depth: 3
number_sections: true
theme: lumen
header-includes:
  - \usepackage{textcolor}
  - \usepackage{xcolor}
---

## Colored Equations

I had this laying around as a gist and decided to include it here. See the source here: 

https://github.com/gfleetwood/gfleetwood.github.io/blob/master/index.Rmd

In particular pay attention to the LaTeX packages in the yaml header (header-includes:).

$\color{blue}{y} = \color{green}{m}\color{red}{x} + \color{orange}{b}$ 

For every one unit change in the <span style="color:red">independent variable</span> there is a <span style="color:green">change</span> in the <span style="color:blue">dependent variable</span> plus some  <span style="color:orange">offset/bias</span> to represent the value of the dependent variable when the independent variable is zero.

For example, consider a mock linear relationship between a person's weight in pounds and their height in inches. 

$\color{blue}{Weight} = \color{green}{2}*\color{red}{Height} + \color{orange}{12}$ 

This says that it is expected that a person's <span style="color:blue">weight</span> will increase by <span style="color:green">2 pounds</span> for every additional inch in their <span style="color:red">height</span>. If a person's height were 0 inches, then their weight would be <span style="color:orange">12 pounds</span>.

From DataCamp's Bayesian course

$P(\color{orange}{\theta}|\color{purple}{D}) = \frac{\color{green}{P(D|\theta)}\times \color{blue}{P(\theta})}{\color{red}{\Sigma{P(D|\theta)} \times P(\theta)}}$ 

<br> 

The probability of <span style="color:orange">different parameter values</span> given <span style="color:purple">some data</span> is <span style="color:green">the likelihood (The relative probability of the data given different parameter values)</span> multiplied by <span style="color:blue">the prior (The probability of different parameters before seeing the data)</span> divided by <span style="color:red">the total sum of the likelihood weighted by the prior</span>.

## Automating Wordle

Recently I stumbled upon the Wordle craze, and lost interest after trying to solve them manually for one or two days. Programmatic attempts held my attention for much longer as I stumbled upon new ways to implement solutions. The most mind-catching one was a method using Z3:

[https://typon.github.io/wordle.html](https://typon.github.io/wordle.html)

My armchair fascination with formal methods in computer science often crossed paths with SAT solvers, but I never really gave them more than a cursory look for the depth involved. This seemed like a good opportunity to dive a bit deeper. 

But that scope only really involved reading the blog and maybe running some lines of code. I need a deeper hook. One eventually came to mind. All the programmatic solutions were only really partially automated. A user would run code, see the wordle feedback, and then make adjustments as needed. I wanted to eliminate the need for adjustments. Everything would be automated.

How to begin. Well, some light RPA was key. I’m not a fan since I think the field is a walking anti-pattern, but it does have its uses. The first snippet of code opened a link to a Wordle clone without the once a day playing limit ([https://hellowordl.net/](https://hellowordl.net/)), and used Python’s pyautogui to enter letters and take a screenshot. Some fiddling around with cropping produced this:

Not bad. Next was the process of detecting letter and colors. Each square is about 55 pixels length and width, so a simple double for loop sufficed. Some googling helped me write this function to get the dominant color in a square:

```python
def get_dominant_color(letter_square):

  img = Image.fromarray(letter_square)
  dom_color = sorted(img.getcolors(2 ** 24), reverse = True)[0][1]

  return(dom_color)
```

Which I saved as a variable:

```python
COLORS = [
("yellow", (228, 218, 0)), 
("gray", (162, 162, 162)), 
("green", (63, 186, 119))
]
```

Of course trying to match colors exactly is just calling for frustration. I used a distance metric instead. 

Next I wanted to use OCR to identify the letters. This easily took up most of my time. Through trial and error I came up with a computer vision pipeline that cleaned up the image above so much that a blind man could see which letters the image contained. Unfortunately neither pytesseract nor the Google Cloud API could. Despair began to set in.

I’m sure you’re now confused. “Why OCR? Aren’t the letters already known since you’re entering the words?” I eventually came to realization as well, and jettisoned (read saved for reference) all that computer vision code.  

I went some way towards writing my own non-Z3 solver before remember my initial goal. Instead I copied over the relevant Z3 code from [https://typon.github.io/wordle.html](https://typon.github.io/wordle.html) and worked to integrate it into my framework. 

My big addition was the logic of how and when to add constraints to the solver. There are seven, two of which are static:

- Only 26 letters (0-25)
- Only five letter words (sourced from Linux’s dictionary)

The other five are dynamic. In parentheses are the color/s they pertain to:

- Word contains letter (green, yellow)
- Wrong position in word for letter (yellow)
- Right position in word for letter (green)
- Word doesn’t contain letter (gray)
- Word contains only one instance of the letter (green, yellow)

The last one was the stickler. When the code is being manually updated it’s easy to enough to wield, but in a fully automated setup it was just breaking. Consider a word like `flood`. If I guessed `flodd` the first d would be gray and the other green. With z3 this would adds constraints that d is both in and not in the word. You immediately see why this is a problem. I need to check if a letter that should be considered was already labeled as not being in the word and then remove that constraint.

This, apparently, is not trivial in z3. The closest method is solver.remove() which removes all constraints from the solver. Not ideal. Armed with my accumulated z3 resources I finally delved deep into its workings, and arrived at an answer. I added this snippet of code to the cases where the letter was green or yellow:

```python
check_if_grey_before = f"letter_{str(letter_info[2])} != {letter_to_index_map[letter_info[0]]}"
constraints = solver.assertions()
new_constraints = [
constraint 
for constraint in constraints 
if constraint.__repr__() != check_if_grey_before
]

if len(constraints) != len(new_constraints): 
  solver.reset()
  solver.add(new_constraints)
```

In a nutshell it checks if the current constraints say the letter wasn’t in the word. If yes, then it filters out that constraint, resets the solver, and then adds back the constraints. I also added logic to the gray condition to skip saying a letter wasn’t in the word if it was already added as being in it.

My solution worked well sometimes, but the solver still broke down. Obviously I attributed this to some wonky logic in updating the solver, so I added logging code to see what constraints are being used on each turn. Looking at the logic led me to the true culprit. Turns out my color detection was not as ironclad as I thought. 

Here’s the full code: [gfleetwood/auto-wordle](https://github.com/gfleetwood/auto-wordle)

## One BI Tool To Rule Them All

I've used many Business Intelligence platforms in my career. They fell into two main buckets.

One contains all the GUI based platforms with all the features for permissions, but severely lacking basic functionality for displaying and interacting with data. Case in point is PowerBI (and AWS Quick Sight) which apparently require you to click everything column in a table you want to include instead of just showing the table you feed to it.

The other bucket is the mirror of the first. Platforms with incredibility flexibility, but the creator would have to put in a lot of effort to add permissions infrastructure. This glove fits Voila for Jupyter and Shiny in R. (As an aside you can get this for shiny through shinyapps.io.)

I thought the third bucket with permissions and flexibility was mythical, some Platonian form that the world is unable to reify. That is until I discovered hex.tech. This early stage startup derives from the second bucket, starting with a Python notebook interface and adding additional functionality. That includes a deep permissions system and features like:

SQL cells to write SQL connect to databasesInteractive charting cells built on top of altairUI cells to add dropdowns, radio buttons, etc.Installing custom packagesSchedulingA rich templating system for Python objectsThat's the Logic section of hex. The App components allows the user to deploy the notebook as a dashboard either in Story or App mode. It's all I ever wanted from a BI Tool.

## Building Local DNS Websites

This is a reference blog on how to build a local DNS.

I use Google Chrome as my main browser, but I've been thinking of switching (or at least trying out) others. This possibility is hampered by Chrome lock-in powered by extensions and bookmarks, with the latter being the main issue.

The initial step involved a browser agnostic bookmark manager. I settled on buku which is cli based with an additional buku server to allow administration through a web interface. The next step was to permanently run buku server on a local port. Easy enough with systemd. Instead of bookmarking or typing 127.0.0.8000 (for example), I wanted to be able to type [buku.me](http://buku.me/) instead. That turned out to be much harder.

All these instructions pertain to Ubuntu 20.04.

Step 1:

Install buku And buku server:  `pip3 install buku[server]`.

Step 2:

Permanently run buku server with system d.

Add this script as an executable to your bin folder. (Mine is called buku_server.) You'll have to find out where buku server is installed with `which bukuserver`.

```
# I'm using port 5555 but choose whichever port you want
/home/gordon/miniconda3/bin/bukuserver run --host 127.0.0.1 --port 5555

```

Next add this service file, `buku_server.service`, to `/etc/systemd/system`.

```
[Unit]
Description=Buku Server

[Service]
User=gordon
WorkingDirectory=/home/gordon/.local/mybin
ExecStart=/bin/bash /home/gordon/.local/mybin/buku_server
Restart=always

[Install]
WantedBy=multi-user.target

```

Start and enable the service.

```
systemctl start buku_server.service
systemctl enable buku_server.service

```

Go to `127.0.0.1:5555` to check that buku server is running.

Step 3:

This was the draw the rest of the owl part for me. I finally understood the process due to this blog:

`https://www.interserver.net/tips/kb/local-domain-names-ubuntu/`

First you need to add a line to your `/etc/hosts` file: `127.0.0.1 buku.me`. Then add this to a configuration file in `/etc/apache2/sites-available` (mine is `000-default.conf`):

```
# ServerName, ProxyPass, and ProxyPasReverse are the entries to change

<VirtualHost *:80>
  ServerAdmin test@test.com
  ServerName buku.me
  ProxyPass / <http://127.0.0.1:5555/>
  ProxyPassReverse / <http://127.0.0.1:5555/>
</VirtualHost>

```

With that I was able to go to [buku.me](http://buku.me/) in my browser to access the buku server.

Bonus:

Even with this setup I still had to manually add urls to buku server. I wanted to have a shortcut, CTRL+B, that would automatically add a url. This solution actually adds whatever is on the clipboard to buku, so it's on me to make sure it's a url.

In Ubuntu I mapped this custom shortcut to CTRL+B: `home/gordon/buku_add.py` where buku_add.py is:

```
import pyperclip
import os

os.system("buku -a {}".format(pyperclip.paste()))

```

buku_add.py is a vanilla Python script instead of an executable because that didn't work. I have no idea why.

## Automatic Audio Books

Ideas on my "Someday" list usually spend a couple of weeks before leaving forever or returning in disgrace (mine not its). "Can I automatically create an audiobook?" had a successful turn around of about three days.

The book I choose was Carroll Quigley's "The Evolution Of Civilizations". I'm at a point where I am militantly electronic with my reading, so the pdf link from Twitter was perfect. Skimming the book I got the sense that I'd prefer to listen rather than read it - a bifurcation that I recently adopted after ignoring audiobooks for years. No audiobook on Amazon's left me only one option. (Well there was the possibility of using an end to end implementation like [Speechify](https://www.getspeechify.com/) but where would be the fun in that?)

Three days from idea to completion is the small, visible part of an enormous glacier. Beneath the surface lies hours of previous research into pdf extraction and text to speech (also speech to text) programs. The pipeline and its implementation almost immediately existed as one entity in my mind. Here was my process:

- Use pdf extraction to get the text

```
library(pdftools)

text_extracted <- pdf_text("auto-audio-book/CarrollQuigley-TheEvolutionOfCivilizations-AnIntroductionToHistoricalAnalysis1979.pdf")

text_concatenated <- paste(text_extracted, collapse = " ")
con <- file("evol_of_civilizations.txt")
writeLines(text_concatenated, con)
close(con)

```

Check.

- Clean the text

So I lied about the automatically part. I still had to go into the document and delete footers, references, the table of contents, etc, and I left stuff in there because it was too tedious to go through a 400+ page book to get every single one. Much easier to ignore them while listening. Alternatively this could be done after the text to speech conversion using a service like [descript](https://www.descript.com/).

- Use text to speech to create the audio

This turned out to be more of a hassle than I thought it would be. First I went through Google's convoluted process to get the Text To Speech API working. Then my attempt at making a single mp3 was thwarted by the API's limits. Chunking the file and adding in a delay (\*What felt like two hours later\*) left me with 180 separate mp3s. (In retrospect I should have used the multiprocess module to speed it up.)

```
from google.cloud import texttospeech
import time

def tts_book(ind, text_chunk):
    # Instantiates a client
    client = texttospeech.TextToSpeechClient()

    # Set the text input to be synthesized
    synthesis_input = texttospeech.types.SynthesisInput(text = text_chunk)

    # Build the voice request, select the language code ("en-US") and the ssml
    # voice gender ("neutral")
    voice = texttospeech.types.VoiceSelectionParams(
        language_code = 'en-US-Wavenet-B',
        ssml_gender = texttospeech.enums.SsmlVoiceGender.NEUTRAL
        )

    # Select the type of audio file you want returned
    audio_config = texttospeech.types.AudioConfig(
        audio_encoding = texttospeech.enums.AudioEncoding.MP3
        )

    # Perform the text-to-speech request on the text input with the selected
    # voice parameters and audio file type
    response = client.synthesize_speech(synthesis_input, voice, audio_config)

    # The response's audio_content is binary.
    with open('output_{}.mp3'.format(ind), 'wb') as out:
        # Write the response to the output file.
        out.write(response.audio_content)
        print('Audio content written to file "output_{}.mp3"'.format(ind))

    return(1)

with open("auto-audio-book/evol-of-civilizations-cleaned.txt", "r") as f:
    evol_civ = f.read().replace("\\n", " ")

chunks = []
temp = evol_civ

while len(temp) != 0:

    chunks.append(temp[:4000])
    temp = temp[4000:]

for ind, chunk in enumerate(chunks):
    _ = tts_book(ind, chunk)
    time.sleep(10)

```

It was a pretty small wrench though. I knew ffmpeg was the solution I needed. The tricky bit was in getting it to work. After some Googling I assembled the parts:

```
# Get all the mp3 names and write them to a text file.
ls | grep "output" > mp3-files.txt

# Prepend the word "file" to each line of said text file.
awk '{print "file " $0}' mp3-files.txt > mp3-files.txt

# Bring in the heavy guns to produce the combined file
ffmpeg -f concat -safe 0 -i mp3-files.txt -c copy output-final.mp3

```

Success!

## Your Keyboard As A Mouse

Setting up a Raspberry Pi can be frustrating if you're not prepared. On the manual route of flashing an SD card and navigating the alpha soup of cable connectors (USB Mini, USB C, HDMI to Mini HDMI), the other potential sticking points are the input and output devices. So it was that one day I dodged all the potential bullets before being painfully shot with a missing mouse. "If only I could use a keyboard as a mouse," put me down the road that led to this article.

First I tried synthesizing this keyboard enhancement drug in Python with pyinput and pyautogui. That worked in testing but I couldn't get it to work with systemd as a permanent application. Someone helpfully pointed out that there were some lower level shenanigans going on, but I wasn't sure how to proceed so I abandoned this work for a while.

I forget the catalyst for its revival, but somehow I came across the exact architecture I needed to use: [1](http://xahlee.info/linux/linux_xbindkeys_tutorial.html) [2](https://anton.logvinenko.site/en/blog/emulation-of-mouse-movements-and-keystrokes.html). xbindkeys and xte became the keys to my success.

```
sudo apt update
sudo apt-get install xbindkeys -y
sudo apt-get install xautomation -y
touch ~/.xbindkeysrc
xbindkeys --key # find name of key
killall -s1 xbindkeys # make xbindkeys reload config
xbindkeys # start xbindkeys daemon
xbindkeys -f ~/.xbindkeysrc
```

And inside the .xbindkeysrc config file:

```
# Keyboard as Mouse

# move mouse up (with alt+w)
"xte 'mousermove 0 -10'"
   alt+w

# move mouse down
"xte 'mousermove 0 10'"
   alt+s

# move mouse left
"xte 'mousermove -10 0'"
   alt+a

# move mouse right
"xte 'mousermove 10 0'"
   alt+d

# left click
"sleep 1 && xte 'mouseclick 1'"
   alt+q

# right click
"sleep 1 && xte 'mouseclick 3'"
   alt+e
```


## GitHub Stars Management

Think of a piece of software you frequently use and a feature wish list will likely follow. My list for GitHub mine includes custom tags for starred repositories. I am a simple man.

GitHub's current support isn't too shabby. There's a language filter, three sorting criteria, and a decent search function which works with (creator supplied) tags. However, the inability to add my own tags to my 1500+ strong collection is the imperfection that ruins the experience. It's a small thing in the possible world of new features GitHub, but it would have an outsized effect on my productivity.

How am I functioning now? There are custom tagging options out there including a chrome extension and web apps like Astral - which served me well. As my number of starred repos grew Astral strained under the weight, and the push for a replacement beckoned.

My initial plan was to build my own web app and scale as needed. As a seemingly continual web dev novice the task seemed a bit daunting, but I threw myself into it. In the midst of reading about designing a schema for tags an idea struck. Why not just use GitHub? GitHub as a CMS has been on my mind since seeing the ingenious use of a [repo's issues as a blog](https://github.com/lukego/blog). The best place to have tagged stars was on GitHub. GitHub issues had tags built in, and an advanced search functionality that put the official starred repos search bar to shame.

The pivot was on.

After some experimentation I teased out the CRUD paradigm for GitHub I needed:

```
from github import Github
import json

g = Github(os.environ["GHUB"])

repo = g.get_repo("gfleetwood/test")

# CREATE: issue
repo.create_issue(title = "This is a new issue 2", body = "This is the issue body", labels = ["python", "magic"])

# READ: issue
iss = repo.get_issue(number = 3)

# UPDATE: issue
iss.edit(title = "Changing Title", body = "<https://example.com>")

# DELETE: issue

# There's no deletion in the GitHub Rest API (strangely it exists in the GraphQL API) so use closing to simulate
iss.edit(state = "closed")

```

The PyGithub package is fairly easy to use as it mirror the GitHub API so directly that you have two sets of documentation to reference. A full CMS workflow would include CRUD for files and repositories. I noted those as well but omitted them here.

I mostly mirrored the info from Astral here. That boiled down to the repo id, name, url, description, and language.

A fresh start would skip this section. I wasn't going to waste all my hard work tagging items in Astral though. The app supports exporting data as json, and after some sleuthing I figured out that repo ids were the (local) unique identifier (apparently on GitHub they're not unique), and massaged the data into a dataframe.

```
import json
import pandas as pd

with open("my_astral_data.json", "r") as f:
    astral_raw_data = f.read()

astral_dict = json.loads(astral_raw_data)

astral_df = pd.DataFrame([
    [astral_dict[x]['repo_id'], ','.join(y['name'] for y in astral_dict[x]['tags'])]
    for x in astral_dict.keys()],
    columns = ['repo_id', 'tags']
)

```

After some validation, combining language and tags, and filling in missing values, it was time for the coup de grace.

```
ghub_star_data = pd.merge(likes_df, astral_df, on = ["repo_id"])
ghub_star_data["tags_lang"] = ghub_star_data["tags"] + "," + ghub_star_data["language"]

ghub_star_data = ghub_star_data.fillna(
    value = {'description': "No Description", 'tags_lang': "no-tag"}
)

```

Asteres is the Greek word for stars. As I'm writing this I realize using the Greek word for sky (Ouranos) would have been more fitting. In any case, the hard work was done. All that was left was to fill the repo's issues. 

```
import time

def create_issue_from_starred_repo_df(df, repo):
    """
    Takes in a dataframe of a starred repos data and makes it a issue
    for the repo
    """

    issue_title = "{} ({})".format(df['name'], df['repo_id'])
    issue_body = "{}\\n\\n{}".format(df['url'], df['description'])
    issue_tags = [tag for tag in df['tags_lang'].split(",")]

    if issue_tags[0] == "no-tag":

        repo.create_issue(
          title = issue_title,
          body = issue_body
          )
    else:
        repo.create_issue(
          title = issue_title,
          body = issue_body,
          labels = issue_tags
          )

    # This is usually processed functionally as a batch so the delay
    # sidesteps GitHub API limiting
    time.sleep(3)

    return(1)

repo = g.get_repo("gfleetwood/asteres")
ghub_star_data["written_to_repo"] = ghub_star_data.apply(lambda x: like_to_issue(x, repo), axis = 1)

```

I added the delay to sidestep an API throttling warning so this took a while. Finally, I locked the issues.

```
# Lock issues
issues = [x for x in repo.get_issues()]
_ = [x.lock("resolved") for x in issues]

```

To keep the repo in sync with the starred collection. In my ideal world GitHub would have web hooks for user actions like starring repositories. In this world I used a GitHub Actions cron job to trigger a Heroku Flask endpoint to update Asteres if needed every 12 hours:

```
# asteres/.github/workflows/curl.yml

name: "Trigger Update"
on:
  schedule:
  - cron: "0 */12 * * *"
jobs:
  curl:
    runs-on: ubuntu-latest
    steps:
    - name: curl
      uses: wei/curl@master
      with:
        args: <https://example.com/>

```

"Every 12 hours?!" Initially it was every hour but meditating on my history of real time tagging made me realize that it wasn't necessary.

Ever so often in building this workflow I stopped to wonder whether or not this was worth no using a "proper" web app solution. GitHub is not a (real time) web database, and I could use the pain of learning how to manage one of those. I went ahead for a couple of reasons. One, as I already mentioned, was that I felt GitHub should be the place where this exists. The second was the interest in this as a proof of concept for using GitHub in an unusual way.
